I"¼<p><em>Presentation of a Paper avalaible <a href="https://arxiv.org/pdf/1912.00953.pdf">here</a></em>:</p>

<p>Training generative adversarial networks demand to match of dainty competitive dynamics. Yet by precise tuning, training may differ or end up in poor stability with dropped modes. In this work, they improved CS-GAN with natural gradient-based latent optimisation and confirm that it increases adversarial dynamics by intensifying interactions between the discriminator and the generator. Their experiments demonstrate that latent optimisation can significantly enhance GAN training, achieving state-of-the-art achievement for the ImageNet (128 Ã— 128) dataset. Their model achieves an Inception Score (IS) of 148 and an FrÃ©chet Inception Distance (FID) of 3.4, an enhancement of 17% and 32% in IS and FID individually, confronted with the baseline BigGAN-deep model with the same architecture and number of parameters.</p>

<!--more-->

<h5 id="introduction">Introduction</h5>

<p>Generative Adversarial Nets (GANs) are inherent generative models that can be trained to satisfy a provided data distribution. GANs was basically stated by Goodfellow et al. (2014) for image data.
As the field of generative modelling has progressed, GANs rest at the edge, generating high fidelity images at massive scale. Nonetheless, despite increasing insights within the dynamics of GAN training, much of the progress in GAN-based image generation happens from network architecture improvements or regularisation of distinct parts of the model.</p>

<h6 id="what-do-they-propose">What do they propose</h6>

<p>They used standard BigGAN-deep architecture with three minor alterations:</p>
<ol>
  <li>They extended the size of the latent source from 128 to 256, to compensate the randomness of the source lost when optimising the latent variable.</li>
  <li>They used the uniform distribution U(âˆ’1, 1) rather than the standard average distribution N(0, 1) for p(z) to be consistent with the clipping operation.</li>
  <li>They used leaky ReLU (with the slope of 0.2 for the negative part) instead of ReLU as the non-linearity for smoother gradient flow consistent with the detailed findings in this paper, their experiment with this baseline model obtains only slightly better scores compared with those here:</li>
</ol>

<p>They computed the FID and IS as in this paper, and calculated IS values from checkpoints with the lowest FIDs.</p>

<p>Finally, they calculated the means and standard deviations for both measures from 5 models with different random seeds.
To apply latent optimisation with NGD, they used the same large step size of Î± = 0.9 as in SN-GAN (section 5.1). However, we found much heavier damping is essential for BigGAN, so we use the damping factor Î² = 5.0, and only optimise 50% of zâ€™s elements. 
Consistent with Tanaka (2019), they found a much larger weight of 300.0 for the regulariser Rz (eq. 17) works best since deeper models generally have larger Lipschitz constants. 
All other hyper-parameters, including learning rates and a large batch size of 2048, remain the same as in BigGAN-deep. They called this model LOGAN (NGD).</p>

<h5 id="contribution">Contribution</h5>

<p>In this work, we present LOGAN, which significantly improves the state of the art in large scale
GAN training for image generation by optimising the latent source z. Our results illustrate improvements in quantitative evaluation and samples with higher quality and diversity. Moreover, our
analysis suggests that LOGAN fundamentally improves adversarial training dynamics. LOGAN is
related to the energy-based formulation of a GANâ€™s discriminator (Dai et al., 2017; Kumar et al.,
2019; Du &amp; Mordatch, 2019), when latent optimisation is viewed as descending the energy function
defined by the discriminator. From this view, sampling from the distribution implicitly defined by
this energy function, via, e.g., Langevin sampling (Welling &amp; Teh, 2011), may bring further benefits. Another class of approaches regularises the entropy of the generator outputs to reduce mode
collapse (Belghazi et al., 2018; Dieng et al., 2019). Such techniques could be combined with LOGAN to further improve coverage of the underlying data distribution. Moreover, we expect our
method to be useful in other tasks that involve adversarial training, including representation learning
and inference (Donahue et al., 2017; Dumoulin et al., 2017; Donahue &amp; Simonyan, 2019), text generation (Zhang et al., 2019), style learning (Zhu et al., 2017; Karras et al., 2019), audio generation
(Donahue et al., 2018) and video generation (Vondrick et al., 2016; Clark et al., 2019).</p>

<h5 id="conclusion">Conclusion</h5>
<p>To review their enrichment:</p>
<ol>
  <li>They propose an advanced, effective approach to latent optimisation using natural gradient descent.</li>
  <li>Their algorithm increases the state-of-the-art BigGAN by a significant margin, without introducing any architectural change, resulting in more astonishing quality images and more different representations.</li>
  <li>To provide theoretical insight, they analysed latent optimisation in GANs from the view of differentiable games. They argue that latent optimisation can be viewed as improving the dynamics of adversarial training.</li>
</ol>

:ET