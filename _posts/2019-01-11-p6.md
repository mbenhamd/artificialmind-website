---
layout: post
title: "Ensemble method applied to Detection of financial fraud"
description: "Implementing Fisher's algorithm (dynamic programming)"
date: 2019-01-11
feature_image: /ressources/ensemble-method/boxplot.png
tags: [Ensemble Method,  SMOTE, Imbalanced Dataset]
---

*Presentation of ensemble method applied to Detection of financial fraud available on my [Github repository](https://github.com/mbenhamd/methodes-ensemblistes-fraudes)*:

# Abstract

Fraud represents a loss of turnover of several billion
dollars and increases every year. The economic crime survey
PwC global survey in 2018 revealed that half (49%) of the 7,200
respondents had been the victim of some type of fraud. It's about
of an increase over the 2016 PwC study in which a little
more than one third of the organizations surveyed (36%) had been victims of
economic crime.
Traditional methods of data analysis have been used for a long time to detect fraud and it is in this context that we will carry out
several ways to approach the problem in order to see the advantages and disadvantages of each learning model and the one that gives the best
prediction results.

<!--more-->

# Introduction

Techniques used to detect fraud require investigation
complex and tedious then they deal with different areas of knowledge such as finance, economics.
Here are some examples of statistical data analysis techniques:
- Pre-processing techniques for detecting, validating, correcting errors and filling missing data
or incorrect.
- Calculation of various statistical parameters such as averages, quantiles, performance measures, probability distributions, etc.
- Chronological analysis of time-dependent data (time series).

![alt text](/ressources/ensemble-method/features.png "MNIST Sample")

In our case, we will use models of supervised learning and
we will not use clustering methods.
This type of model only detects frauds similar to those
that took place previously and were classified by a human.
Regarding the detection of fraud by credit card payment,
the problem of ranking involves the creation of sufficiently intelligent models to properly classify transactions into legitimate transactions
or fraudulent, depending on the details of the transaction such as the amount,
the trader, the location, the time and others.

# Conclusion

To conclude, we have been very interested in the set-up methods in our approach to this problem in order to be original.
The decision tree-based methods are interesting because
model architecture reduces over-fitting, but poor sampling practices can lead to misleading conclusions
the quality of a model.
The main purpose of model validation is to estimate how
whose model will generalize to new data. If the decision to put
a model in production depends on how it works on a
validation set, it is essential that the oversampling be performed
correctly.
In fact, by oversampling only the training data, none of the information contained in the validation data is used
to create synthetic observations therefore, these results should be
generalizable.
Then oversampling is a well-known way to potentially improve the models formed on unbalanced data but it is important to remember that incorrect oversampling can lead to
think that a model will generalize better than it actually does.
Our results may seem weak compared to what is available on the internet but it is appropriate to use cross validation correctly with
good sampling method as well as an appropriate metric when
our data is out of balance.