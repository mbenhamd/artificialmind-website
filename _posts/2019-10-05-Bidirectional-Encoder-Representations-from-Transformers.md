---
layout: post
title: "Bidirectional Encoder Representations from Transformers."
description: "state-of-the-art in natural language processing by using a Transformer."
date: 2019-10-05
tags: [BERT, NLP, state-of-the-art]
---
<!---
-->


<!---
Few important words
-->

*Explanation of the model which upset the natural processing of language. [Publication](https://arxiv.org/pdf/1810.04805.pdf)*\

According to the researchers who designed BERT was thought to pre-form deep two-way representations from unlabeled text and jointly conditioning the left and right contexts. The result gives us that a pre-trained model can be refined by adding an additional superficial layer to meet the NLP task.

<!--more-->

<!---
Write an intro (and make it captivating).
-->

##### Who is BERT ? 

No, it is not the first name as you could have understood it thanks to the title and the small introduction but beautiful and well a framework which can help you to solve your NLP projects. Coming from the Language department at Google Research and it has inspired many architectures known for example the OpenAI GPT-2 model.

##### Why did we come up with this?

We had some problems with previous methods because language models only use left context or right context, but language understanding is bidirectional. 
There are two main reasons why language models are unidirectional : 
Directionality is necessitated to make a well-formed probability pattern. The second idea is that information can be seen in a bidirectional encoder.


##### BERT Explained

There are useful articles on that subject that you should read. I would advice two of them that cover fundamentals and other questions. 
[The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)
[BERT Explained â€“ A list of Frequently Asked Questions](https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/#:~:text=What%20is%20BERT%3F,task%2Dspecific%20fine%2Dtuning.)

