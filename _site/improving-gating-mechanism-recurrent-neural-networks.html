<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta name="google-site-verification" content="jk_P6ub99gOREVqLFGd54Xe1Dgp57gAS248_qlRzIJg" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Improving the Gating Mechanism of Recurrent Neural Networks</title>
  <meta name="description" content="Gating mechanisms are widely used in neural network models, where they allow gradients to backpropagate more easily through depth or time. However, their sat...">
  <link rel="canonical" href="http://localhost:4000/improving-gating-mechanism-recurrent-neural-networks">
  <link rel="alternate" type="application/rss+xml" title="Artificial Mind Blog. Feed"
    href="http://localhost:4000/feed.xml">
  
  <!-- Styles -->
  <link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700,700i%7CNoto+Serif:400,400i,700,700i&display=swap" rel="stylesheet">
  <link href="/assets/css/style.css" rel="stylesheet">
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Improving the Gating Mechanism of Recurrent Neural Networks | Artificial Mind Blog.</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Improving the Gating Mechanism of Recurrent Neural Networks" />
<meta name="author" content="Mohamed BEN HAMDOUNE" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Gating mechanisms are widely used in neural network models, where they allow gradients to backpropagate more easily through depth or time. However, their saturation property introduces problems of its own." />
<meta property="og:description" content="Gating mechanisms are widely used in neural network models, where they allow gradients to backpropagate more easily through depth or time. However, their saturation property introduces problems of its own." />
<link rel="canonical" href="http://localhost:4000/improving-gating-mechanism-recurrent-neural-networks" />
<meta property="og:url" content="http://localhost:4000/improving-gating-mechanism-recurrent-neural-networks" />
<meta property="og:site_name" content="Artificial Mind Blog." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-10-30T00:00:00+00:00" />
<meta name="google-site-verification" content="jk_P6ub99gOREVqLFGd54Xe1Dgp57gAS248_qlRzIJg" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Improving the Gating Mechanism of Recurrent Neural Networks","dateModified":"2019-10-30T00:00:00+00:00","datePublished":"2019-10-30T00:00:00+00:00","url":"http://localhost:4000/improving-gating-mechanism-recurrent-neural-networks","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/improving-gating-mechanism-recurrent-neural-networks"},"author":{"@type":"Person","name":"Mohamed BEN HAMDOUNE"},"description":"Gating mechanisms are widely used in neural network models, where they allow gradients to backpropagate more easily through depth or time. However, their saturation property introduces problems of its own.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>

  <div id="page" class="site">
    <div class="inner">
      <header class="site-header">
  
  <p class="site-title"><a class="logo-text" href="/">Artificial Mind Blog.</a></p>
  
  <nav class="site-navigation">
    <div class="site-navigation-wrap">
      <h2 class="screen-reader-text">Main navigation</h2>
      <ul class="menu">
        
        
        
        <li class="menu-item ">
          <a class="" href="/">Home</a>
        </li>
        
        
        
        <li class="menu-item ">
          <a class="" href="/curriculum-vitae/">Curriculum Vitae</a>
        </li>
        
        
        
        <li class="menu-item ">
          <a class="" href="/tags/">Archive</a>
        </li>
        
        
        
        <li class="menu-item ">
          <a class="" href="/about/">About</a>
        </li>
        
      </ul><!-- .menu -->
      <button id="menu-close" class="menu-toggle"><span class="screen-reader-text">Close Menu</span><span
          class="icon-close" aria-hidden="true"></span></button>
    </div><!-- .site-navigation-wrap -->
  </nav><!-- .site-navigation -->
  <button id="menu-open" class="menu-toggle"><span class="screen-reader-text">Open Menu</span><span class="icon-menu" aria-hidden="true"></span></button>
</header>




      <main class="main-content fadeInDown delay_075s">

  <article class="post">
    <header class="post-header">
      <time class="post-date" datetime="2019-10-30">October 30, 2019</time>
      <h1 class="post-title">Improving the Gating Mechanism of Recurrent Neural Networks</h1>
      <div class="post-meta">
        By <span class="post-author">Mohamed BEN HAMDOUNE</span><span class="post-tags"> in <a href="/tags/#Recurrent+Neural+Networks" rel="tag">Recurrent Neural Networks</a>, <a href="/tags/#Associative+memory" rel="tag">Associative memory</a>, <a href="/tags/#Gradient-Based+Meta-Learning" rel="tag">Gradient-Based Meta-Learning</a></span>
      </div><!-- .post-meta -->
      
      <figure class="post-thumbnail image-card width-wide">
        <img src="https://miro.medium.com/max/1121/1*voNageVB1gI8Dfrmr-U3Ew.png" alt="Improving the Gating Mechanism of Recurrent Neural Networks">
      </figure><!-- .post-thumbnail -->
      
    </header><!-- .post-header -->
    <div class="post-content">
      <p><em>Presentation of a Paper avalaible <a href="https://arxiv.org/pdf/1910.09890.pdf">here</a></em>:</p>

<p>Gating mechanisms are widely used in neural
network models, where they allow gradients to
backpropagate more easily through depth or time.
However, their saturation property introduces
problems of its own. For example, in recurrent
models these gates need to have outputs near 1
to propagate information over long time-delays,
which requires them to operate in their saturation
regime and hinders gradient-based learning of
the gate mechanism. We address this problem by
deriving two synergistic modifications to the standard gating mechanism that are easy to implement,
introduce no additional hyperparameters, and
improve learnability of the gates when they are
close to saturation. We show how these changes
are related to and improve on alternative recently
proposed gating mechanisms such as chrono
initialization and Ordered Neurons. Empirically,
our simple gating mechanisms robustly improve
the performance of recurrent models on a range
of applications, including synthetic memorization
tasks, sequential image classification, language
modeling, and reinforcement learning, particularly
when long-term dependencies are involved.
<!--more--></p>

<h5 id="introduction">Introduction</h5>
<p>Recurrent neural networks (RNNs) are an established
machine learning tool for learning from sequential data.
However, RNNs are prone to the vanishing gradient problem,
which occurs when the gradients of the recurrent weights become vanishingly small as they get backpropagated through
time (Hochreiter, 1991; Bengio et al., 1994; Hochreiter
et al., 2001). A common approach to alleviate the vanishing
gradient problem is to use gating mechanisms, leading to
models such as the long short term memory (Hochreiter
&amp; Schmidhuber, 1997, LSTM) and gated recurrent units
1
Stanford University, USA 2DeepMind, London, UK. Correspondence to: Albert Gu <a href="mailto:albertgu@stanford.edu">albertgu@stanford.edu</a>, Caglar Gulcehre <a href="mailto:caglarg@google.com">caglarg@google.com</a>.
Proceedings of the 37 th International Conference on Machine
Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by
the author(s).
(Chung et al., 2014, GRUs). These gated RNNs have been
very successful in several different application areas such
as in reinforcement learning (Kapturowski et al., 2018;
Espeholt et al., 2018) and natural language processing
(Bahdanau et al., 2014; Kocisk Ë‡ y et al. ` , 2018).</p>
<h6 id="what-do-they-propose">What do they propose</h6>

<p>The URLSTM requires two small modifications to the
vanilla LSTM. First, we present the way the biases of forget
gates are initialized in Equation (12) with UGI. Second,
the modifications on the standard LSTM equations to
compute the refine and effective forget gates are presented
in Equations (9)-(11). However, we note that these methods
can be used to modify any gate (or more generally, bounded
function) in any model. In this context, the URLSTM is
simply defined by applying UGI and a refine gate r on the
original forget gate f to create an effective forget gate g
(Equation (10)). This effective gate is then used in the cell
state update (11). Empirically, these small modifications
to an LSTM are enough to allow it to achieve nearly binary
activations and solve difficult memory problems (Figure 5).</p>

<h5 id="method">Method</h5>

<p>We first perform full ablations of the gating variants (Section 4.1) on two common benchmarks for testing memory
models: synthetic memory tasks and pixel-by-pixel image
classification tasks. We then evaluate our main method
on important applications for recurrent models including
language modeling and reinforcement learning, comparing
against baselines from literature where appropriate.
The main claims we evaluate for each gating component
are (i) the refine gate is more effective than alternatives
(the master gate, or no auxiliary gate), and (ii) UGI is more
effective than standard initialization for sigmoid gates. In
particular, we expect the <em>R gate to be more effective than
*M or *- for any primary gate *, and we expect U</em> to be
better than -* and comparable to O* for any auxiliary gate *.</p>

<h5 id="result">Result</h5>
<p>In this work, we introduce and evaluate several modifications
to the ubiquitous gating mechanism that appears in recurrent
neural networks. We describe methods that improve
on the standard gating method by alleviating problems
with initialization and optimization. The mechanisms
considered include changes on independent axes, namely
initialization/activations and auxiliary gates, and we perform
extensive ablations on our improvements with previously
considered modifications. Our main gate model robustly
improves on standard gates across many different tasks and
recurrent cores, while requiring less tuning. Finally, we
emphasize that these improvements are entirely independent
of the large body of research on neural network architectures
that use gates, and hope that these insights can be applied
to improve machine learning models at large.</p>

    </div><!-- .post-content -->
    <div class="post-share">
      <span>Share:</span>
      <a target="_blank"
        href="https://twitter.com/intent/tweet?text=Improving%20the%20Gating%20Mechanism%20of%20Recurrent%20Neural%20Networks&amp;url=http://localhost:4000/improving-gating-mechanism-recurrent-neural-networks" rel="noopener">Twitter</a>
      <a target="_blank"
        href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/improving-gating-mechanism-recurrent-neural-networks&amp;t=Improving%20the%20Gating%20Mechanism%20of%20Recurrent%20Neural%20Networks" rel="noopener">Facebook</a>
    </div><!-- .share-post -->
    <div class="author-box">
      
      <div class="author-avatar" style="background-image: url('/images/author.png')"><span class="screen-reader-text">Mohamed BEN HAMDOUNE's Picture</span></div>
      
      <div class="author-details">
        <h2 class="author-title">About Mohamed BEN HAMDOUNE</h2>
        <div class="author-bio"><p>Mohamed is a Data Scientist working at Zifo RnD Solutions.</p>
</div>
        
        <span class="author-location">Cmabridge, United Kingdom</span>
        
        
        <span class="author-website"><a href="https://mbenhamd.github.io/" target="_blank" rel="noopener">https://mbenhamd.github.io/</a></span>
        
      </div><!-- .author-details -->
    </div><!-- .author-box -->
  </article><!-- .post -->

  
    <div class="comments-area">
  <div class="comments-inner">
    <h2 class="comments-title">Comments</h2>
    <div id="disqus_thread"></div>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
        Disqus</a>.</noscript>
  </div><!-- .comments-inner -->
</div><!-- .comments-area -->

<script type="text/javascript">
  var disqus_shortname = 'justgoodthemes';
  var disqus_developer = 0;
  (function () {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
  

</main><!-- .main-content -->
      <footer class="site-footer">
  <div class="offsite-links">
    
      
<a href="https://twitter.com/mbenhamdtw" target="_blank" rel="noopener">
  <span class="fa-twitter" aria-hidden="true"></span>
  <span class="screen-reader-text">Twitter</span>
</a>

<a href="https://github.com/mbenhamd" target="_blank" rel="noopener">
  <span class="fa-github" aria-hidden="true"></span>
  <span class="screen-reader-text">GitHub</span>
</a>

<a href="https://www.linkedin.com/in/mohamed-ben-hamdoune-73197392/" target="_blank" rel="noopener">
  <span class="fa-linkedin" aria-hidden="true"></span>
  <span class="screen-reader-text">LinkedIn</span>
</a>

    
  </div><!-- .offsite-links -->
  <div class="footer-bottom">
    <div class="site-info">
      <p>Artificial Mind Blog. Jekyll Theme Forked <a href="https://github.com/JustGoodThemes/Scriptor-Jekyll-Theme">Here</a>.</p>

    </div><!-- .site-info -->
    <a href="#page" id="back-to-top" class="back-to-top"><span class="screen-reader-text">Back to the top </span>&#8593;</a>
  </div><!-- .footer-bottom -->
</footer><!-- .site-footer -->

    </div><!-- .inner -->
  </div><!-- .site -->

  <!-- Scripts -->
  <script src="/assets/js/plugins.js"></script>
  <script src="/assets/js/custom.js"></script>

</body>
</html>
