<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta name="google-site-verification" content="jk_P6ub99gOREVqLFGd54Xe1Dgp57gAS248_qlRzIJg" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Meta-Learning Deep Energy-Based Memory Models</title>
  <meta name="description" content="They study the problem of learning associative memory -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.">
  <link rel="canonical" href="http://localhost:4000/meta-learning-deep-energy-based-memory-models">
  <link rel="alternate" type="application/rss+xml" title="Artificial Mind Blog. Feed"
    href="http://localhost:4000/feed.xml">
  
  <!-- Styles -->
  <link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700,700i%7CNoto+Serif:400,400i,700,700i&display=swap" rel="stylesheet">
  <link href="/assets/css/style.css" rel="stylesheet">
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Meta-Learning Deep Energy-Based Memory Models | Artificial Mind Blog.</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Meta-Learning Deep Energy-Based Memory Models" />
<meta name="author" content="Mohamed BEN HAMDOUNE" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="They study the problem of learning associative memory – a system which is able to retrieve a remembered pattern based on its distorted or incomplete version." />
<meta property="og:description" content="They study the problem of learning associative memory – a system which is able to retrieve a remembered pattern based on its distorted or incomplete version." />
<link rel="canonical" href="http://localhost:4000/meta-learning-deep-energy-based-memory-models" />
<meta property="og:url" content="http://localhost:4000/meta-learning-deep-energy-based-memory-models" />
<meta property="og:site_name" content="Artificial Mind Blog." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-10-21T00:00:00+01:00" />
<meta name="google-site-verification" content="jk_P6ub99gOREVqLFGd54Xe1Dgp57gAS248_qlRzIJg" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Meta-Learning Deep Energy-Based Memory Models","dateModified":"2019-10-21T00:00:00+01:00","datePublished":"2019-10-21T00:00:00+01:00","url":"http://localhost:4000/meta-learning-deep-energy-based-memory-models","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/meta-learning-deep-energy-based-memory-models"},"author":{"@type":"Person","name":"Mohamed BEN HAMDOUNE"},"description":"They study the problem of learning associative memory – a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>

  <div id="page" class="site">
    <div class="inner">
      <header class="site-header">
  
  <p class="site-title"><a class="logo-text" href="/">Artificial Mind Blog.</a></p>
  
  <nav class="site-navigation">
    <div class="site-navigation-wrap">
      <h2 class="screen-reader-text">Main navigation</h2>
      <ul class="menu">
        
        
        
        <li class="menu-item ">
          <a class="" href="/">Home</a>
        </li>
        
        
        
        <li class="menu-item ">
          <a class="" href="/curriculum-vitae/">Curriculum Vitae</a>
        </li>
        
        
        
        <li class="menu-item ">
          <a class="" href="/tags/">Archive</a>
        </li>
        
        
        
        <li class="menu-item ">
          <a class="" href="/about/">About</a>
        </li>
        
      </ul><!-- .menu -->
      <button id="menu-close" class="menu-toggle"><span class="screen-reader-text">Close Menu</span><span
          class="icon-close" aria-hidden="true"></span></button>
    </div><!-- .site-navigation-wrap -->
  </nav><!-- .site-navigation -->
  <button id="menu-open" class="menu-toggle"><span class="screen-reader-text">Open Menu</span><span class="icon-menu" aria-hidden="true"></span></button>
</header>




      <main class="main-content fadeInDown delay_075s">

  <article class="post">
    <header class="post-header">
      <time class="post-date" datetime="2019-10-21">October 21, 2019</time>
      <h1 class="post-title">Meta-Learning Deep Energy-Based Memory Models</h1>
      <div class="post-meta">
        By <span class="post-author">Mohamed BEN HAMDOUNE</span><span class="post-tags"> in <a href="/tags/#Energy-based+Memory+Models" rel="tag">Energy-based Memory Models</a>, <a href="/tags/#Associative+memory" rel="tag">Associative memory</a>, <a href="/tags/#Gradient-Based+Meta-Learning" rel="tag">Gradient-Based Meta-Learning</a>, <a href="/tags/#EBMM" rel="tag">EBMM</a></span>
      </div><!-- .post-meta -->
      
      <figure class="post-thumbnail image-card width-wide">
        <img src="https://yt3.ggpht.com/a/AGF-l7-ncmSiLyMlXHexWBJfa61xH8Y02WWQbnI4rg=s900-c-k-c0xffffffff-no-rj-mo" alt="Meta-Learning Deep Energy-Based Memory Models">
      </figure><!-- .post-thumbnail -->
      
    </header><!-- .post-header -->
    <div class="post-content">
      <p><em>Presentation of a Paper avalaible <a href="https://arxiv.org/pdf/1910.02720.pdf">here</a></em>:</p>

<p>We study the problem of learning associative memory – a system which is able to retrieve a remembered pattern based on its distorted or incomplete version. Attractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. In such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. In general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast. Thus, most research in energybased memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing. We present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. We demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.
<!--more--></p>

<h5 id="introduction">Introduction</h5>

<p>Associative memory has long been of interest to neuroscience and machine learning communities (Willshaw et al., 1969; Hopfield, 1982; Kanerva, 1988). This interest has generated many proposals for associative memory models, both biological and synthetic. These models address the problem of storing a set of patterns in such a way that a stored pattern can be retrieved based on a partially known or distorted version. This kind of retrieval from memory is known as auto-association.</p>

<h6 id="what-do-they-propose">What do they propose</h6>

<p>In this section we experimentally evaluate EBMM on a number of real-world image datasets.
The performance of EBMM is compared to a set of relevant baselines: Long-Short Term Memory (LSTM) (Hochreiter &amp; Schmidhuber, 1997), the classical Hopfield network (Hopfield, 1982), Memory-Augmented Neural Networks (MANN) (Santoro et al., 2016) (which are a variant of the Differentiable Neural Computer (Graves et al., 2016)), Memory Networks (Weston et al., 2014), Differentiable Plasticity model of Miconi et al. (2018) (a generalization of the Fast-weights RNN (Ba et al., 2016)) and Dynamic Kanerva Machine (Wu et al., 2018). Some of these baselines failed to learn at all for real-world images. In the Appendix A.2 we provide additional experiments with random binary strings with a larger set of representative models. The experimental procedure is the following: we write a fixed-sized batch of images into a memory model, then corrupt a random block of the written image to form a query and let the model retrieve the originally stored image. By varying the memory size and repeating this procedure, we perform distortion/rate analysis, i.e. we measure how well a memory model can retrieve a remembered pattern for a given memory size.</p>

<h5 id="method">Method</h5>

<p>Deep neural networks are capable of both compression (Parkhi et al., 2015; Kraska et al., 2018),
and memorizing training patterns (Zhang et al., 2016). Taken together, these properties make
deep networks an attractive candidate for memory models, with both exact recall and compressive
capabilities. However, there exists a natural trade-off between the speed of writing and the realizable
capacity of a model (Ba et al., 2016). Approaches similar to ours in their use of gradient descent
dynamics, but lacking fast writing, have been proposed by Hinton et al. (2006a) and recently revisited
by Du &amp; Mordatch (2019) together with another stochastic deep energy model (Krotov &amp; Hopfield,
2016). In general it is difficult to derive a writing rule for a given dynamics equation or an energy
model which we attempt to address in this work.
We introduced a novel learning method for deep associative memory systems. Our method benefits
from the recent progress in deep learning so that we can use a very large class of neural networks both
8
Preprint
for learning representations and for storing patterns in network weights. At the same time, we are not
bound by slow gradient learning thanks to meta-learning of fast writing rules. We showed that our
method is applicable in a variety of domains from non-compressible (binary strings; see Appendix)
to highly compressible (natural images) and that the resulting memory system uses available capacity
efficiently. We believe that more elaborate architecture search could lead to stronger results on par
with state-of-the-art generative models.
The existing limitation of EBMM is the batch writing assumption, which is in principle possible to
relax. This would enable embedding of the model in reinforcement learning agents or into other tasks
requiring online-updating memory. It would be also interesting to explore a stochastic variant of
EBMM that could return different associations in the presence of uncertainty caused by compression.
Finally, many general principles of learning attractor models with desired properties are yet to be
discovered and we believe that our results provide a good motivation for this line of research.</p>

    </div><!-- .post-content -->
    <div class="post-share">
      <span>Share:</span>
      <a target="_blank"
        href="https://twitter.com/intent/tweet?text=Meta-Learning%20Deep%20Energy-Based%20Memory%20Models&amp;url=http://localhost:4000/meta-learning-deep-energy-based-memory-models" rel="noopener">Twitter</a>
      <a target="_blank"
        href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/meta-learning-deep-energy-based-memory-models&amp;t=Meta-Learning%20Deep%20Energy-Based%20Memory%20Models" rel="noopener">Facebook</a>
    </div><!-- .share-post -->
    <div class="author-box">
      
      <div class="author-avatar" style="background-image: url('/images/author.png')"><span class="screen-reader-text">Mohamed BEN HAMDOUNE's Picture</span></div>
      
      <div class="author-details">
        <h2 class="author-title">About Mohamed BEN HAMDOUNE</h2>
        <div class="author-bio"><p>Mohamed is a Data Scientist working at Zifo RnD Solutions.</p>
</div>
        
        <span class="author-location">Cmabridge, United Kingdom</span>
        
        
        <span class="author-website"><a href="https://mbenhamd.github.io/" target="_blank" rel="noopener">https://mbenhamd.github.io/</a></span>
        
      </div><!-- .author-details -->
    </div><!-- .author-box -->
  </article><!-- .post -->

  
    <div class="comments-area">
  <div class="comments-inner">
    <h2 class="comments-title">Comments</h2>
    <div id="disqus_thread"></div>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
        Disqus</a>.</noscript>
  </div><!-- .comments-inner -->
</div><!-- .comments-area -->

<script type="text/javascript">
  var disqus_shortname = 'justgoodthemes';
  var disqus_developer = 0;
  (function () {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
  
  <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T8VZJBT');</script>
<!-- End Google Tag Manager -->
  <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8VZJBT"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
</main><!-- .main-content -->
      <footer class="site-footer">
  <div class="offsite-links">
    
      
<a href="https://twitter.com/mbenhamdtw" target="_blank" rel="noopener">
  <span class="fa-twitter" aria-hidden="true"></span>
  <span class="screen-reader-text">Twitter</span>
</a>

<a href="https://github.com/mbenhamd" target="_blank" rel="noopener">
  <span class="fa-github" aria-hidden="true"></span>
  <span class="screen-reader-text">GitHub</span>
</a>

<a href="https://www.linkedin.com/in/mohamed-ben-hamdoune-73197392/" target="_blank" rel="noopener">
  <span class="fa-linkedin" aria-hidden="true"></span>
  <span class="screen-reader-text">LinkedIn</span>
</a>

    
  </div><!-- .offsite-links -->
  <div class="footer-bottom">
    <div class="site-info">
      <p>Artificial Mind Blog. Jekyll Theme Forked <a href="https://github.com/JustGoodThemes/Scriptor-Jekyll-Theme">Here</a>.</p>

    </div><!-- .site-info -->
    <a href="#page" id="back-to-top" class="back-to-top"><span class="screen-reader-text">Back to the top </span>&#8593;</a>
  </div><!-- .footer-bottom -->
</footer><!-- .site-footer -->

    </div><!-- .inner -->
  </div><!-- .site -->

  <!-- Scripts -->
  <script src="/assets/js/plugins.js"></script>
  <script src="/assets/js/custom.js"></script>
  <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T8VZJBT');</script>
<!-- End Google Tag Manager -->
  <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8VZJBT"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
</body>
</html>
