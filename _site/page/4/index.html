<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta name="google-site-verification" content="jk_P6ub99gOREVqLFGd54Xe1Dgp57gAS248_qlRzIJg" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Artificial Mind Blog.</title>
  <meta name="description" content="Welcome to my website, it covers a section regarding my vocation, but there are articles about my projects and some about new ideas in the field of artificia...">
  <link rel="canonical" href="http://localhost:4000/page/4/">
  <link rel="alternate" type="application/rss+xml" title="Artificial Mind Blog. Feed"
    href="http://localhost:4000/feed.xml">
  
  <!-- Styles -->
  <link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700,700i%7CNoto+Serif:400,400i,700,700i&display=swap" rel="stylesheet">
  <link href="/assets/css/style.css" rel="stylesheet">
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Page 4 of 9 for Artificial Mind Blog. | Welcome to my website, it covers a section regarding my vocation, but there are articles about my projects and some about new ideas in the field of artificial intelligence.</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Artificial Mind Blog." />
<meta name="author" content="Mohamed BEN HAMDOUNE" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Welcome to my website, it covers a section regarding my vocation, but there are articles about my projects and some about new ideas in the field of artificial intelligence." />
<meta property="og:description" content="Welcome to my website, it covers a section regarding my vocation, but there are articles about my projects and some about new ideas in the field of artificial intelligence." />
<link rel="canonical" href="http://localhost:4000/page/4/" />
<meta property="og:url" content="http://localhost:4000/page/4/" />
<meta property="og:site_name" content="Artificial Mind Blog." />
<link rel="prev" href="http://localhost:4000/page/3" />
<link rel="next" href="http://localhost:4000/page/5" />
<meta name="google-site-verification" content="jk_P6ub99gOREVqLFGd54Xe1Dgp57gAS248_qlRzIJg" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/page/4/","headline":"Artificial Mind Blog.","author":{"@type":"Person","name":"Mohamed BEN HAMDOUNE"},"description":"Welcome to my website, it covers a section regarding my vocation, but there are articles about my projects and some about new ideas in the field of artificial intelligence.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>

  <div id="page" class="site">
    <div class="inner">
      <header class="site-header">
  
  <p class="site-title"><a class="logo-text" href="/">Artificial Mind Blog.</a></p>
  
  <nav class="site-navigation">
    <div class="site-navigation-wrap">
      <h2 class="screen-reader-text">Main navigation</h2>
      <ul class="menu">
        
        
        
        <li class="menu-item ">
          <a class="" href="/">Home</a>
        </li>
        
        
        
        <li class="menu-item ">
          <a class="" href="/curriculum-vitae/">Curriculum Vitae</a>
        </li>
        
        
        
        <li class="menu-item ">
          <a class="" href="/tags/">Archive</a>
        </li>
        
        
        
        <li class="menu-item ">
          <a class="" href="/about/">About</a>
        </li>
        
      </ul><!-- .menu -->
      <button id="menu-close" class="menu-toggle"><span class="screen-reader-text">Close Menu</span><span
          class="icon-close" aria-hidden="true"></span></button>
    </div><!-- .site-navigation-wrap -->
  </nav><!-- .site-navigation -->
  <button id="menu-open" class="menu-toggle"><span class="screen-reader-text">Open Menu</span><span class="icon-menu" aria-hidden="true"></span></button>
</header>




      <main class="main-content fadeInDown delay_075s">

  
  <article class="post">
    <header class="post-header">
      <time class="post-date" datetime="2019-10-07">October 7, 2019</time>
      <h2 class="post-title"><a href="/gated-linear-networks" rel="bookmark">Gated Linear Networks</a></h2>
      <div class="post-meta">
        By <span class="post-author">Mohamed BEN HAMDOUNE</span><span
          class="post-tags"> in
          <a href="/tags/#Backpropagation" rel="tag">Backpropagation</a>, <a href="/tags/#Gated+Linear+Networks" rel="tag">Gated Linear Networks</a>, <a href="/tags/#Probabilistic+Models" rel="tag">Probabilistic Models</a>, <a href="/tags/#Gated+Geometric+Mixture" rel="tag">Gated Geometric Mixture</a></span>
      </div><!-- .post-meta -->
      
      <figure class="post-thumbnail image-card width-wide">
        <a href="/gated-linear-networks"><img src="https://yt3.ggpht.com/a/AGF-l7-ncmSiLyMlXHexWBJfa61xH8Y02WWQbnI4rg=s900-c-k-c0xffffffff-no-rj-mo"
            alt="Gated Linear Networks"></a>
      </figure><!-- .post-thumbnail -->
      
    </header><!-- .post-header -->
    <div class="post-content">
      
      <p><em>Presentation of a Paper avalaible <a href="https://arxiv.org/pdf/1910.01526.pdf">here</a></em>:</p>

<p>This paper presents a family of backpropagation-free neural architectures, Gated Linear Networks (GLNs), that are well suited to online learning applications where sample efficiency is of paramount importance. The impressive empirical performance of these architectures has long been known within the data compression community, but a theoretically satisfying explanation as to how and why they perform so well has proven difficult.
It’s written by Joel Veness, Tor Lattimore, Avishkar Bhoopchand, David Budden, Christopher Mattern, Agnieszka Grabska-Barwinska, Peter Toth, Simon Schmitt and Marcus Hutter from DeepMind.

      <p class="read-more"><a href="/gated-linear-networks" class="read-more-link">Continue reading &rarr;</a>
      </p>
      
    </div><!-- .post-content -->
  </article><!-- .post -->
  
  <article class="post">
    <header class="post-header">
      <time class="post-date" datetime="2019-10-05">October 5, 2019</time>
      <h2 class="post-title"><a href="/Bidirectional-Encoder-Representations-from-Transformers" rel="bookmark">Bidirectional Encoder Representations from Transformers.</a></h2>
      <div class="post-meta">
        By <span class="post-author">Mohamed BEN HAMDOUNE</span><span
          class="post-tags"> in
          <a href="/tags/#BERT" rel="tag">BERT</a>, <a href="/tags/#NLP" rel="tag">NLP</a>, <a href="/tags/#state-of-the-art" rel="tag">state-of-the-art</a>, <a href="/tags/#Tranformers" rel="tag">Tranformers</a></span>
      </div><!-- .post-meta -->
      
      <figure class="post-thumbnail image-card width-wide">
        <a href="/Bidirectional-Encoder-Representations-from-Transformers"><img src="https://github.com/mbenhamd/artificialmind-website/blob/master/images/Bert_smile.png?raw=true"
            alt="Bidirectional Encoder Representations from Transformers."></a>
      </figure><!-- .post-thumbnail -->
      
    </header><!-- .post-header -->
    <div class="post-content">
      
      <!---
-->

<!---
Few important words
-->

<p><em>Explanation of the model which upset the natural processing of language. <a href="https://arxiv.org/pdf/1810.04805.pdf">Publication</a></em></p>

<p>According to the researchers who designed BERT was thought to pre-form deep two-way representations from unlabeled text and jointly conditioning the left and right contexts. The result gives us that a pre-trained model can be refined by adding an additional superficial layer to meet the NLP task.</p>


      <p class="read-more"><a href="/Bidirectional-Encoder-Representations-from-Transformers" class="read-more-link">Continue reading &rarr;</a>
      </p>
      
    </div><!-- .post-content -->
  </article><!-- .post -->
  
  <article class="post">
    <header class="post-header">
      <time class="post-date" datetime="2019-10-01">October 1, 2019</time>
      <h2 class="post-title"><a href="/weakly-supervised-adaptive-triplet-loss" rel="bookmark">A weakly supervised adaptive triplet loss for deep metric learning</a></h2>
      <div class="post-meta">
        By <span class="post-author">Mohamed BEN HAMDOUNE</span><span
          class="post-tags"> in
          <a href="/tags/#Loss" rel="tag">Loss</a>, <a href="/tags/#Weakly+Supervised" rel="tag">Weakly Supervised</a>, <a href="/tags/#Learning+Metrics" rel="tag">Learning Metrics</a>, <a href="/tags/#Triplet+Loss" rel="tag">Triplet Loss</a>, <a href="/tags/#Semantic+Classes" rel="tag">Semantic Classes</a></span>
      </div><!-- .post-meta -->
      
      <figure class="post-thumbnail image-card width-wide">
        <a href="/weakly-supervised-adaptive-triplet-loss"><img src="https://miro.medium.com/max/1204/0*_WNBFcRVEOz6QM7R."
            alt="A weakly supervised adaptive triplet loss for deep metric learning"></a>
      </figure><!-- .post-thumbnail -->
      
    </header><!-- .post-header -->
    <div class="post-content">
      
      <p><em>Presentation of a Paper avalaible <a href="https://arxiv.org/pdf/1909.12939.pdf">here</a></em>:</p>

<p>The article presents an approach to the major problems of learning metrics on the search for similarities in the case of image data sets. The authors present a method called a weakly supervised adaptrive triplet loss (ATL) that can capture fine-grained semantic similarity.
It’s written by Xiaonan Zthao, Hian Qi, Rui Luo and Larry Davis from Amazon R&amp;D.

      <p class="read-more"><a href="/weakly-supervised-adaptive-triplet-loss" class="read-more-link">Continue reading &rarr;</a>
      </p>
      
    </div><!-- .post-content -->
  </article><!-- .post -->
  

  
  <nav class="pagination">
    <h2 class="screen-reader-text">Posts navigation</h2>
    <div class="nav-links">
      
      <a href="/page/3" class="newer-posts">&larr; Newer Posts</a>
      
      <span class="page-number">Page 4 of 9</span>
      
      <a href="/page/5" class="older-posts">Older Posts &rarr;</a>
      
    </div>
  </nav><!-- .pagination -->
  

</main><!-- .site-main -->
      <footer class="site-footer">
  <div class="offsite-links">
    
      
<a href="https://twitter.com/mbenhamdtw" target="_blank" rel="noopener">
  <span class="fa-twitter" aria-hidden="true"></span>
  <span class="screen-reader-text">Twitter</span>
</a>

<a href="https://github.com/mbenhamd" target="_blank" rel="noopener">
  <span class="fa-github" aria-hidden="true"></span>
  <span class="screen-reader-text">GitHub</span>
</a>

<a href="https://www.linkedin.com/in/mohamed-ben-hamdoune-73197392/" target="_blank" rel="noopener">
  <span class="fa-linkedin" aria-hidden="true"></span>
  <span class="screen-reader-text">LinkedIn</span>
</a>

    
  </div><!-- .offsite-links -->
  <div class="footer-bottom">
    <div class="site-info">
      <p>Artificial Mind Blog. Jekyll Theme Forked <a href="https://github.com/JustGoodThemes/Scriptor-Jekyll-Theme">Here</a>.</p>

    </div><!-- .site-info -->
    <a href="#page" id="back-to-top" class="back-to-top"><span class="screen-reader-text">Back to the top </span>&#8593;</a>
  </div><!-- .footer-bottom -->
</footer><!-- .site-footer -->

    </div><!-- .inner -->
  </div><!-- .site -->

  <!-- Scripts -->
  <script src="/assets/js/plugins.js"></script>
  <script src="/assets/js/custom.js"></script>
  <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T8VZJBT');</script>
<!-- End Google Tag Manager -->
  <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8VZJBT"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
</body>
</html>
